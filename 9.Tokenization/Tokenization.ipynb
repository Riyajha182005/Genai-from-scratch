{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKhybIcKe3C36QykKDhUpS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riyajha182005/Genai-from-scratch/blob/main/9.Tokenization/Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a36d19a6"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "**Definition:** Tokenization is the process of breaking down a sentence or a paragraph into smaller pieces called tokens. Think of tokens as the individual building blocks of text.\n",
        "\n",
        "**Easy Explanation:** Imagine you have a sentence like \"The quick brown fox jumps over the lazy dog.\" Tokenization is like taking that sentence and separating it into each word: \"The\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"the\", \"lazy\", \"dog\". Each of these words is a token.\n",
        "\n",
        "We do this because computers understand text better when it's broken down into these smaller units. It's like taking a big puzzle and breaking it into individual pieces so you can work with each piece separately. Tokenization is a crucial first step in many tasks that involve working with text, like analyzing what a text is about or translating languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ_sB5u7C-Ic",
        "outputId": "f1ed6f08-bd0f-4430-e632-3577c311bd7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = '''\n",
        "    This is the first sentence of the corpus.\n",
        "    This is the second sentence, which is a bit longer.\n",
        "    And this is the third and final sentence.\n",
        "    Here is another sentence for good measure.\n",
        "    '''\n",
        "\n",
        "\n",
        "print(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUkpweYAGWLG",
        "outputId": "455e8e0c-7ffc-40b2-bf28-2cfb1cac423f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is the first sentence of the corpus.\n",
            "    This is the second sentence, which is a bit longer.\n",
            "    And this is the third and final sentence.\n",
            "    Here is another sentence for good measure.\n",
            "    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenization\n",
        "## sentence-> paragraph\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc_3-wcSGvQm",
        "outputId": "e816a4f9-4bb5-4fa6-e9b2-6e39f49efd20"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vY-gWFsrLASN",
        "outputId": "7f30517c-401e-4161-fe80-8f74cb7ce45c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    This is the first sentence of the corpus.',\n",
              " 'This is the second sentence, which is a bit longer.',\n",
              " 'And this is the third and final sentence.',\n",
              " 'Here is another sentence for good measure.']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Tokenization\n",
        "## paragraph--> words\n",
        "from nltk.tokenize import word_tokenize\n",
        "word_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJzIyh9jMIdh",
        "outputId": "21b494ff-7267-4ee8-caa3-a65740eebf7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " 'of',\n",
              " 'the',\n",
              " 'corpus',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'sentence',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'a',\n",
              " 'bit',\n",
              " 'longer',\n",
              " '.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'and',\n",
              " 'final',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'Here',\n",
              " 'is',\n",
              " 'another',\n",
              " 'sentence',\n",
              " 'for',\n",
              " 'good',\n",
              " 'measure',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## sentence --> words Tokenization\n",
        "for sentence in sent_tokenize(corpus):\n",
        "  print(sentence)\n",
        "  print(word_tokenize(sentence))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySnPjcSUNE2D",
        "outputId": "f6411fd7-4abd-4697-ac70-555f2030de8a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    This is the first sentence of the corpus.\n",
            "['This', 'is', 'the', 'first', 'sentence', 'of', 'the', 'corpus', '.']\n",
            "This is the second sentence, which is a bit longer.\n",
            "['This', 'is', 'the', 'second', 'sentence', ',', 'which', 'is', 'a', 'bit', 'longer', '.']\n",
            "And this is the third and final sentence.\n",
            "['And', 'this', 'is', 'the', 'third', 'and', 'final', 'sentence', '.']\n",
            "Here is another sentence for good measure.\n",
            "['Here', 'is', 'another', 'sentence', 'for', 'good', 'measure', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de993dce"
      },
      "source": [
        "### Difference between `word_tokenize` and `WordPunctTokenizer`\n",
        "\n",
        "Both `word_tokenize` and `WordPunctTokenizer` are tokenization methods available in NLTK, but they handle punctuation differently.\n",
        "\n",
        "*   **`word_tokenize`:** This is a more sophisticated tokenization method that generally separates words and punctuation marks, but it tries to keep contractions and other special cases together as single tokens (e.g., \"don't\" might be tokenized as \"don't\").\n",
        "\n",
        "*   **`WordPunctTokenizer`:** This tokenizer is simpler and splits tokens based on whitespace and punctuation. It treats punctuation as separate tokens. For example, \"don't\" would be tokenized as \"don\", \"'\", and \"t\".\n",
        "\n",
        "Let's look at an example to see the difference:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWq344M6N6s_",
        "outputId": "3185ddf9-3131-454e-a982-cd51b6e177c0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'first',\n",
              " 'sentence',\n",
              " 'of',\n",
              " 'the',\n",
              " 'corpus',\n",
              " '.',\n",
              " 'This',\n",
              " 'is',\n",
              " 'the',\n",
              " 'second',\n",
              " 'sentence',\n",
              " ',',\n",
              " 'which',\n",
              " 'is',\n",
              " 'a',\n",
              " 'bit',\n",
              " 'longer',\n",
              " '.',\n",
              " 'And',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'third',\n",
              " 'and',\n",
              " 'final',\n",
              " 'sentence',\n",
              " '.',\n",
              " 'Here',\n",
              " 'is',\n",
              " 'another',\n",
              " 'sentence',\n",
              " 'for',\n",
              " 'good',\n",
              " 'measure',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}